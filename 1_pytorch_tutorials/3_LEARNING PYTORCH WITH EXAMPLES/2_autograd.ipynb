{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors and autograd\n",
    "\n",
    "- In the above examples, we had to manually implement both the forward and backward passes of our neural network. \n",
    "- Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "- we can use [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) to automate the computation of backward passes in neural networks. \n",
    "- The ``autograd`` package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a ``computational graph``; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. \n",
    "- Backpropagating through this graph then allows you to easily compute gradients.\n",
    "- Each Tensor represents a node in a computational graph. If ``x`` is a Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor holding the gradient of ``x`` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33016882.0\n",
      "1 28244012.0\n",
      "2 26021324.0\n",
      "3 22972844.0\n",
      "4 18206440.0\n",
      "5 12823937.0\n",
      "6 8235750.5\n",
      "7 5075709.5\n",
      "8 3156311.25\n",
      "9 2055365.625\n",
      "10 1424860.25\n",
      "11 1050842.25\n",
      "12 816193.6875\n",
      "13 658883.0625\n",
      "14 546431.8125\n",
      "15 461623.53125\n",
      "16 394924.5\n",
      "17 341099.75\n",
      "18 296758.1875\n",
      "19 259686.125\n",
      "20 228379.21875\n",
      "21 201686.890625\n",
      "22 178797.296875\n",
      "23 159050.890625\n",
      "24 141920.15625\n",
      "25 126995.6953125\n",
      "26 113897.390625\n",
      "27 102414.1015625\n",
      "28 92319.5859375\n",
      "29 83416.8984375\n",
      "30 75542.171875\n",
      "31 68560.7734375\n",
      "32 62351.4453125\n",
      "33 56823.9296875\n",
      "34 51889.76953125\n",
      "35 47469.15234375\n",
      "36 43498.45703125\n",
      "37 39921.6796875\n",
      "38 36693.50390625\n",
      "39 33776.64453125\n",
      "40 31137.333984375\n",
      "41 28744.044921875\n",
      "42 26568.146484375\n",
      "43 24585.978515625\n",
      "44 22778.412109375\n",
      "45 21125.57421875\n",
      "46 19613.091796875\n",
      "47 18227.0390625\n",
      "48 16955.6015625\n",
      "49 15786.9638671875\n",
      "50 14711.498046875\n",
      "51 13720.3486328125\n",
      "52 12806.6669921875\n",
      "53 11962.546875\n",
      "54 11182.0830078125\n",
      "55 10461.283203125\n",
      "56 9793.609375\n",
      "57 9174.5078125\n",
      "58 8599.662109375\n",
      "59 8065.6953125\n",
      "60 7569.2734375\n",
      "61 7107.22216796875\n",
      "62 6676.9765625\n",
      "63 6276.59228515625\n",
      "64 5903.1845703125\n",
      "65 5554.67919921875\n",
      "66 5229.06103515625\n",
      "67 4924.8515625\n",
      "68 4640.322265625\n",
      "69 4374.015625\n",
      "70 4124.6279296875\n",
      "71 3890.885009765625\n",
      "72 3671.746826171875\n",
      "73 3466.30078125\n",
      "74 3273.492919921875\n",
      "75 3092.404296875\n",
      "76 2922.303466796875\n",
      "77 2762.409423828125\n",
      "78 2612.09814453125\n",
      "79 2470.632080078125\n",
      "80 2337.520751953125\n",
      "81 2212.315673828125\n",
      "82 2094.398681640625\n",
      "83 1983.1947021484375\n",
      "84 1878.3677978515625\n",
      "85 1779.541259765625\n",
      "86 1686.2974853515625\n",
      "87 1598.3048095703125\n",
      "88 1515.2652587890625\n",
      "89 1436.87451171875\n",
      "90 1362.85009765625\n",
      "91 1293.1231689453125\n",
      "92 1227.307861328125\n",
      "93 1165.071044921875\n",
      "94 1106.20556640625\n",
      "95 1050.541259765625\n",
      "96 997.8718872070312\n",
      "97 948.00634765625\n",
      "98 900.8274536132812\n",
      "99 856.11669921875\n",
      "100 813.7470092773438\n",
      "101 773.6102905273438\n",
      "102 735.55712890625\n",
      "103 699.5029907226562\n",
      "104 665.3140869140625\n",
      "105 632.8923950195312\n",
      "106 602.13623046875\n",
      "107 572.9740600585938\n",
      "108 545.2781372070312\n",
      "109 519.0094604492188\n",
      "110 494.0846252441406\n",
      "111 470.4167785644531\n",
      "112 447.9103698730469\n",
      "113 426.556396484375\n",
      "114 406.2605285644531\n",
      "115 386.99298095703125\n",
      "116 368.6632995605469\n",
      "117 351.2526550292969\n",
      "118 334.697509765625\n",
      "119 318.96319580078125\n",
      "120 303.9934387207031\n",
      "121 289.7603454589844\n",
      "122 276.2274475097656\n",
      "123 263.3584899902344\n",
      "124 251.11196899414062\n",
      "125 239.4547119140625\n",
      "126 228.36480712890625\n",
      "127 217.80628967285156\n",
      "128 207.75384521484375\n",
      "129 198.18499755859375\n",
      "130 189.07679748535156\n",
      "131 180.40264892578125\n",
      "132 172.14068603515625\n",
      "133 164.26815795898438\n",
      "134 156.77053833007812\n",
      "135 149.6275177001953\n",
      "136 142.82371520996094\n",
      "137 136.3434600830078\n",
      "138 130.1678009033203\n",
      "139 124.28189849853516\n",
      "140 118.67254638671875\n",
      "141 113.32489776611328\n",
      "142 108.22699737548828\n",
      "143 103.36628723144531\n",
      "144 98.72966766357422\n",
      "145 94.30648040771484\n",
      "146 90.08931732177734\n",
      "147 86.06413269042969\n",
      "148 82.2283935546875\n",
      "149 78.56764221191406\n",
      "150 75.07488250732422\n",
      "151 71.74076080322266\n",
      "152 68.56103515625\n",
      "153 65.52355194091797\n",
      "154 62.62648391723633\n",
      "155 59.85958480834961\n",
      "156 57.218318939208984\n",
      "157 54.69789505004883\n",
      "158 52.29123306274414\n",
      "159 49.99313735961914\n",
      "160 47.79883575439453\n",
      "161 45.70306396484375\n",
      "162 43.70313262939453\n",
      "163 41.79092788696289\n",
      "164 39.96435546875\n",
      "165 38.220672607421875\n",
      "166 36.55488204956055\n",
      "167 34.964664459228516\n",
      "168 33.444007873535156\n",
      "169 31.99640655517578\n",
      "170 30.61351776123047\n",
      "171 29.291561126708984\n",
      "172 28.028202056884766\n",
      "173 26.821353912353516\n",
      "174 25.667346954345703\n",
      "175 24.563936233520508\n",
      "176 23.50944709777832\n",
      "177 22.500953674316406\n",
      "178 21.53717803955078\n",
      "179 20.614856719970703\n",
      "180 19.73305320739746\n",
      "181 18.89014434814453\n",
      "182 18.08361053466797\n",
      "183 17.312679290771484\n",
      "184 16.575902938842773\n",
      "185 15.870504379272461\n",
      "186 15.195615768432617\n",
      "187 14.550081253051758\n",
      "188 13.932328224182129\n",
      "189 13.341157913208008\n",
      "190 12.77562141418457\n",
      "191 12.234977722167969\n",
      "192 11.717409133911133\n",
      "193 11.22216796875\n",
      "194 10.748204231262207\n",
      "195 10.29511547088623\n",
      "196 9.860931396484375\n",
      "197 9.445222854614258\n",
      "198 9.047666549682617\n",
      "199 8.666749954223633\n",
      "200 8.302594184875488\n",
      "201 7.954000473022461\n",
      "202 7.619955539703369\n",
      "203 7.300585746765137\n",
      "204 6.994467258453369\n",
      "205 6.701664924621582\n",
      "206 6.420994281768799\n",
      "207 6.153007507324219\n",
      "208 5.895320892333984\n",
      "209 5.6491522789001465\n",
      "210 5.413422107696533\n",
      "211 5.187370777130127\n",
      "212 4.971346855163574\n",
      "213 4.764246940612793\n",
      "214 4.565677642822266\n",
      "215 4.375837326049805\n",
      "216 4.193541526794434\n",
      "217 4.019482135772705\n",
      "218 3.85255765914917\n",
      "219 3.6925783157348633\n",
      "220 3.539459466934204\n",
      "221 3.3925559520721436\n",
      "222 3.252119779586792\n",
      "223 3.1171793937683105\n",
      "224 2.9882357120513916\n",
      "225 2.8646960258483887\n",
      "226 2.746098756790161\n",
      "227 2.632446765899658\n",
      "228 2.5239083766937256\n",
      "229 2.419663667678833\n",
      "230 2.319854497909546\n",
      "231 2.2242634296417236\n",
      "232 2.1325135231018066\n",
      "233 2.044707775115967\n",
      "234 1.9604294300079346\n",
      "235 1.8797712326049805\n",
      "236 1.802475094795227\n",
      "237 1.728424072265625\n",
      "238 1.6572661399841309\n",
      "239 1.589008092880249\n",
      "240 1.523819088935852\n",
      "241 1.4612783193588257\n",
      "242 1.401193618774414\n",
      "243 1.343745231628418\n",
      "244 1.288657784461975\n",
      "245 1.2358591556549072\n",
      "246 1.185381531715393\n",
      "247 1.1367889642715454\n",
      "248 1.090311050415039\n",
      "249 1.0454984903335571\n",
      "250 1.0028146505355835\n",
      "251 0.9618483781814575\n",
      "252 0.9225622415542603\n",
      "253 0.8848058581352234\n",
      "254 0.8487144708633423\n",
      "255 0.8140661716461182\n",
      "256 0.7808185815811157\n",
      "257 0.7489414215087891\n",
      "258 0.7184377312660217\n",
      "259 0.6891577243804932\n",
      "260 0.6610836982727051\n",
      "261 0.6340928077697754\n",
      "262 0.6083423495292664\n",
      "263 0.5835239887237549\n",
      "264 0.5596969723701477\n",
      "265 0.5369217991828918\n",
      "266 0.5151489973068237\n",
      "267 0.49419230222702026\n",
      "268 0.4741078317165375\n",
      "269 0.45483869314193726\n",
      "270 0.4363435208797455\n",
      "271 0.4185876250267029\n",
      "272 0.4016707241535187\n",
      "273 0.3853556215763092\n",
      "274 0.3697066903114319\n",
      "275 0.3547629117965698\n",
      "276 0.34032487869262695\n",
      "277 0.3265395164489746\n",
      "278 0.31335845589637756\n",
      "279 0.30067962408065796\n",
      "280 0.28845474123954773\n",
      "281 0.27677568793296814\n",
      "282 0.26558318734169006\n",
      "283 0.25483736395835876\n",
      "284 0.2445235550403595\n",
      "285 0.2345893830060959\n",
      "286 0.2251439392566681\n",
      "287 0.21602600812911987\n",
      "288 0.20729073882102966\n",
      "289 0.19894254207611084\n",
      "290 0.1908550262451172\n",
      "291 0.18316207826137543\n",
      "292 0.17582881450653076\n",
      "293 0.16867847740650177\n",
      "294 0.1618964970111847\n",
      "295 0.15536139905452728\n",
      "296 0.1490965038537979\n",
      "297 0.14306136965751648\n",
      "298 0.13730977475643158\n",
      "299 0.13176965713500977\n",
      "300 0.12646806240081787\n",
      "301 0.1213955283164978\n",
      "302 0.11649184674024582\n",
      "303 0.11181308329105377\n",
      "304 0.10729233175516129\n",
      "305 0.10299694538116455\n",
      "306 0.0988442674279213\n",
      "307 0.09486521780490875\n",
      "308 0.09104732424020767\n",
      "309 0.08739257603883743\n",
      "310 0.08389030396938324\n",
      "311 0.08049556612968445\n",
      "312 0.07729470729827881\n",
      "313 0.0741875097155571\n",
      "314 0.07117877900600433\n",
      "315 0.06832858920097351\n",
      "316 0.06559570878744125\n",
      "317 0.06294388324022293\n",
      "318 0.060412537306547165\n",
      "319 0.05801304057240486\n",
      "320 0.05568357929587364\n",
      "321 0.05345948413014412\n",
      "322 0.05131373554468155\n",
      "323 0.04925081878900528\n",
      "324 0.04727920517325401\n",
      "325 0.04540296271443367\n",
      "326 0.043598026037216187\n",
      "327 0.041851501911878586\n",
      "328 0.0401722677052021\n",
      "329 0.03856085613369942\n",
      "330 0.03703083470463753\n",
      "331 0.035549141466617584\n",
      "332 0.03413332253694534\n",
      "333 0.03278232365846634\n",
      "334 0.031470563262701035\n",
      "335 0.03020670637488365\n",
      "336 0.02900010347366333\n",
      "337 0.02785385400056839\n",
      "338 0.02675768919289112\n",
      "339 0.025685636326670647\n",
      "340 0.02467133104801178\n",
      "341 0.023685544729232788\n",
      "342 0.022740833461284637\n",
      "343 0.021840998902916908\n",
      "344 0.02096313051879406\n",
      "345 0.02014291100203991\n",
      "346 0.019345209002494812\n",
      "347 0.018589071929454803\n",
      "348 0.017861243337392807\n",
      "349 0.017152536660432816\n",
      "350 0.01648438721895218\n",
      "351 0.015832990407943726\n",
      "352 0.01521045621484518\n",
      "353 0.014603513292968273\n",
      "354 0.014033411629498005\n",
      "355 0.013482601381838322\n",
      "356 0.012959394603967667\n",
      "357 0.012445161119103432\n",
      "358 0.011961150914430618\n",
      "359 0.011490791104733944\n",
      "360 0.01104990765452385\n",
      "361 0.010621110908687115\n",
      "362 0.010209575295448303\n",
      "363 0.00982164777815342\n",
      "364 0.009440255351364613\n",
      "365 0.009070553816854954\n",
      "366 0.008727180771529675\n",
      "367 0.008393196389079094\n",
      "368 0.008069855161011219\n",
      "369 0.007757536601275206\n",
      "370 0.007460794877260923\n",
      "371 0.007176914252340794\n",
      "372 0.006902148947119713\n",
      "373 0.006644402164965868\n",
      "374 0.0063949632458388805\n",
      "375 0.006147101987153292\n",
      "376 0.005915162153542042\n",
      "377 0.005697084590792656\n",
      "378 0.005480917636305094\n",
      "379 0.005274839699268341\n",
      "380 0.005077546928077936\n",
      "381 0.004890069365501404\n",
      "382 0.004709388129413128\n",
      "383 0.004530947655439377\n",
      "384 0.0043597957119345665\n",
      "385 0.004201896023005247\n",
      "386 0.004047260619699955\n",
      "387 0.0039027496241033077\n",
      "388 0.0037605520337820053\n",
      "389 0.0036214508581906557\n",
      "390 0.003491699229925871\n",
      "391 0.0033697050530463457\n",
      "392 0.003247049869969487\n",
      "393 0.003127512289211154\n",
      "394 0.003017483977600932\n",
      "395 0.002913939766585827\n",
      "396 0.0028122970834374428\n",
      "397 0.0027123282197862864\n",
      "398 0.002616321900859475\n",
      "399 0.0025226471479982138\n",
      "400 0.00243573822081089\n",
      "401 0.0023519392125308514\n",
      "402 0.002273116260766983\n",
      "403 0.00219496781937778\n",
      "404 0.002119350479915738\n",
      "405 0.0020468432921916246\n",
      "406 0.001978957559913397\n",
      "407 0.0019155233167111874\n",
      "408 0.0018503962783142924\n",
      "409 0.0017894105985760689\n",
      "410 0.001731176977045834\n",
      "411 0.0016741552390158176\n",
      "412 0.001617989270016551\n",
      "413 0.0015650956192985177\n",
      "414 0.0015159600879997015\n",
      "415 0.0014673369005322456\n",
      "416 0.001421216526068747\n",
      "417 0.0013763172319158912\n",
      "418 0.0013321683509275317\n",
      "419 0.0012939309235662222\n",
      "420 0.0012525306083261967\n",
      "421 0.0012140516191720963\n",
      "422 0.001176698599010706\n",
      "423 0.0011396817862987518\n",
      "424 0.0011059076059609652\n",
      "425 0.0010737591655924916\n",
      "426 0.001041562296450138\n",
      "427 0.0010096658952534199\n",
      "428 0.000979950767941773\n",
      "429 0.0009506569476798177\n",
      "430 0.0009241769439540803\n",
      "431 0.0008951930212788284\n",
      "432 0.0008695753058418632\n",
      "433 0.0008455903152935207\n",
      "434 0.0008215192356146872\n",
      "435 0.0007977532222867012\n",
      "436 0.0007768452633172274\n",
      "437 0.0007544660475105047\n",
      "438 0.0007350828382186592\n",
      "439 0.0007132849423214793\n",
      "440 0.0006942283362150192\n",
      "441 0.0006750632892362773\n",
      "442 0.0006575396400876343\n",
      "443 0.000639958365354687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 0.0006219332572072744\n",
      "445 0.0006043183384463191\n",
      "446 0.0005899432580918074\n",
      "447 0.0005747248651459813\n",
      "448 0.0005593239911831915\n",
      "449 0.0005458110244944692\n",
      "450 0.0005309059051796794\n",
      "451 0.000517076812684536\n",
      "452 0.0005032930639572442\n",
      "453 0.0004898622864857316\n",
      "454 0.00047815105062909424\n",
      "455 0.00046553235733881593\n",
      "456 0.00045333156595006585\n",
      "457 0.00044187105959281325\n",
      "458 0.00043106861994601786\n",
      "459 0.00042093993397429585\n",
      "460 0.0004116682684980333\n",
      "461 0.000401783938286826\n",
      "462 0.0003928134683519602\n",
      "463 0.00038355126162059605\n",
      "464 0.00037498067831620574\n",
      "465 0.0003666254342533648\n",
      "466 0.00035774457501247525\n",
      "467 0.0003488239599391818\n",
      "468 0.0003410040808375925\n",
      "469 0.00033281443757005036\n",
      "470 0.00032502118847332895\n",
      "471 0.0003175628080498427\n",
      "472 0.0003107347874902189\n",
      "473 0.0003032810927834362\n",
      "474 0.000296848826110363\n",
      "475 0.0002904311113525182\n",
      "476 0.00028415065025910735\n",
      "477 0.00027800502721220255\n",
      "478 0.00027128131478093565\n",
      "479 0.00026688817888498306\n",
      "480 0.0002605086483526975\n",
      "481 0.00025459466269239783\n",
      "482 0.00024918498820625246\n",
      "483 0.00024454339290969074\n",
      "484 0.00023945674183778465\n",
      "485 0.0002340323117095977\n",
      "486 0.00022960071510169655\n",
      "487 0.00022477179300040007\n",
      "488 0.00022019566677045077\n",
      "489 0.0002154861285816878\n",
      "490 0.00021036296675447375\n",
      "491 0.00020638381829485297\n",
      "492 0.00020228906942065805\n",
      "493 0.00019849929958581924\n",
      "494 0.00019502970098983496\n",
      "495 0.00019156202324666083\n",
      "496 0.00018815718067344278\n",
      "497 0.00018443008593749255\n",
      "498 0.00018061856098938733\n",
      "499 0.0001767837384250015\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Defining new autograd functions\n",
    "\n",
    "- Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "- In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "- In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU1(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35078848.0\n",
      "1 33475930.0\n",
      "2 32745788.0\n",
      "3 28216562.0\n",
      "4 19994878.0\n",
      "5 11816578.0\n",
      "6 6383971.0\n",
      "7 3542082.0\n",
      "8 2187618.75\n",
      "9 1523508.625\n",
      "10 1160604.5\n",
      "11 933829.9375\n",
      "12 774863.5\n",
      "13 654107.375\n",
      "14 558172.3125\n",
      "15 479918.8125\n",
      "16 415001.15625\n",
      "17 360588.25\n",
      "18 314639.4375\n",
      "19 275581.125\n",
      "20 242258.703125\n",
      "21 213680.109375\n",
      "22 189024.09375\n",
      "23 167652.171875\n",
      "24 149067.171875\n",
      "25 132857.59375\n",
      "26 118685.0625\n",
      "27 106240.609375\n",
      "28 95281.34375\n",
      "29 85616.5703125\n",
      "30 77079.6484375\n",
      "31 69500.2734375\n",
      "32 62764.08984375\n",
      "33 56758.30078125\n",
      "34 51391.0234375\n",
      "35 46595.0390625\n",
      "36 42296.890625\n",
      "37 38442.734375\n",
      "38 34978.3984375\n",
      "39 31856.94921875\n",
      "40 29042.162109375\n",
      "41 26501.189453125\n",
      "42 24203.388671875\n",
      "43 22124.9296875\n",
      "44 20242.310546875\n",
      "45 18535.19140625\n",
      "46 16985.013671875\n",
      "47 15576.4677734375\n",
      "48 14293.6103515625\n",
      "49 13128.0556640625\n",
      "50 12066.3046875\n",
      "51 11097.3466796875\n",
      "52 10212.6806640625\n",
      "53 9405.4404296875\n",
      "54 8666.2978515625\n",
      "55 7989.140625\n",
      "56 7368.8701171875\n",
      "57 6799.8984375\n",
      "58 6278.3193359375\n",
      "59 5799.7841796875\n",
      "60 5360.16845703125\n",
      "61 4955.90234375\n",
      "62 4584.19921875\n",
      "63 4242.07421875\n",
      "64 3927.137939453125\n",
      "65 3637.0078125\n",
      "66 3369.779296875\n",
      "67 3123.300048828125\n",
      "68 2895.85888671875\n",
      "69 2685.96533203125\n",
      "70 2492.20166015625\n",
      "71 2313.243408203125\n",
      "72 2147.83251953125\n",
      "73 1994.9449462890625\n",
      "74 1853.5936279296875\n",
      "75 1722.738037109375\n",
      "76 1601.5919189453125\n",
      "77 1489.41162109375\n",
      "78 1385.5731201171875\n",
      "79 1289.3316650390625\n",
      "80 1200.09423828125\n",
      "81 1117.357666015625\n",
      "82 1040.6385498046875\n",
      "83 969.4689331054688\n",
      "84 903.361328125\n",
      "85 841.998046875\n",
      "86 785.0123901367188\n",
      "87 732.0818481445312\n",
      "88 682.8759765625\n",
      "89 637.1795654296875\n",
      "90 594.6854858398438\n",
      "91 555.15869140625\n",
      "92 518.4028930664062\n",
      "93 484.179443359375\n",
      "94 452.3041687011719\n",
      "95 422.63238525390625\n",
      "96 395.00592041015625\n",
      "97 369.4365539550781\n",
      "98 345.71087646484375\n",
      "99 323.58697509765625\n",
      "100 302.96075439453125\n",
      "101 283.72369384765625\n",
      "102 265.79144287109375\n",
      "103 249.0390167236328\n",
      "104 233.39524841308594\n",
      "105 218.7829132080078\n",
      "106 205.12847900390625\n",
      "107 192.36798095703125\n",
      "108 180.4380645751953\n",
      "109 169.28245544433594\n",
      "110 158.85052490234375\n",
      "111 149.09803771972656\n",
      "112 139.96697998046875\n",
      "113 131.42005920410156\n",
      "114 123.42436218261719\n",
      "115 115.93244934082031\n",
      "116 108.91598510742188\n",
      "117 102.34449005126953\n",
      "118 96.18515014648438\n",
      "119 90.414794921875\n",
      "120 85.00642395019531\n",
      "121 79.9344482421875\n",
      "122 75.18047332763672\n",
      "123 70.72149658203125\n",
      "124 66.53742980957031\n",
      "125 62.610504150390625\n",
      "126 58.926719665527344\n",
      "127 55.47034454345703\n",
      "128 52.22460174560547\n",
      "129 49.17635726928711\n",
      "130 46.31421661376953\n",
      "131 43.62419509887695\n",
      "132 41.09782028198242\n",
      "133 38.723655700683594\n",
      "134 36.492191314697266\n",
      "135 34.39448928833008\n",
      "136 32.42253875732422\n",
      "137 30.568763732910156\n",
      "138 28.82465362548828\n",
      "139 27.184492111206055\n",
      "140 25.641399383544922\n",
      "141 24.18861198425293\n",
      "142 22.821840286254883\n",
      "143 21.53632354736328\n",
      "144 20.325458526611328\n",
      "145 19.18592643737793\n",
      "146 18.11168670654297\n",
      "147 17.100236892700195\n",
      "148 16.147491455078125\n",
      "149 15.24965763092041\n",
      "150 14.40410041809082\n",
      "151 13.607393264770508\n",
      "152 12.856106758117676\n",
      "153 12.148228645324707\n",
      "154 11.480390548706055\n",
      "155 10.850640296936035\n",
      "156 10.256706237792969\n",
      "157 9.696466445922852\n",
      "158 9.168028831481934\n",
      "159 8.66940689086914\n",
      "160 8.19880485534668\n",
      "161 7.754734516143799\n",
      "162 7.335228443145752\n",
      "163 6.939548492431641\n",
      "164 6.565785884857178\n",
      "165 6.213103771209717\n",
      "166 5.879917621612549\n",
      "167 5.565253257751465\n",
      "168 5.267940521240234\n",
      "169 4.987016677856445\n",
      "170 4.721712112426758\n",
      "171 4.470969200134277\n",
      "172 4.233829975128174\n",
      "173 4.009807109832764\n",
      "174 3.7980964183807373\n",
      "175 3.5982449054718018\n",
      "176 3.4086995124816895\n",
      "177 3.2296276092529297\n",
      "178 3.0604002475738525\n",
      "179 2.9002792835235596\n",
      "180 2.748555898666382\n",
      "181 2.605294942855835\n",
      "182 2.469712734222412\n",
      "183 2.34138560295105\n",
      "184 2.2198238372802734\n",
      "185 2.1047258377075195\n",
      "186 1.996055006980896\n",
      "187 1.8929095268249512\n",
      "188 1.79534912109375\n",
      "189 1.7029743194580078\n",
      "190 1.6154390573501587\n",
      "191 1.5326513051986694\n",
      "192 1.454214334487915\n",
      "193 1.379839301109314\n",
      "194 1.3093905448913574\n",
      "195 1.2426739931106567\n",
      "196 1.1794358491897583\n",
      "197 1.1194943189620972\n",
      "198 1.0626333951950073\n",
      "199 1.0089097023010254\n",
      "200 0.9577792882919312\n",
      "201 0.9094099998474121\n",
      "202 0.8636350631713867\n",
      "203 0.82012939453125\n",
      "204 0.7788565754890442\n",
      "205 0.7398480772972107\n",
      "206 0.7027056217193604\n",
      "207 0.6676368117332458\n",
      "208 0.6342017650604248\n",
      "209 0.6024826765060425\n",
      "210 0.5724823474884033\n",
      "211 0.543967068195343\n",
      "212 0.5169426798820496\n",
      "213 0.49130529165267944\n",
      "214 0.466863751411438\n",
      "215 0.4437378942966461\n",
      "216 0.42175206542015076\n",
      "217 0.4009435474872589\n",
      "218 0.38117003440856934\n",
      "219 0.3623824715614319\n",
      "220 0.3445303738117218\n",
      "221 0.3275829255580902\n",
      "222 0.3114997446537018\n",
      "223 0.2962692677974701\n",
      "224 0.28181779384613037\n",
      "225 0.268028199672699\n",
      "226 0.25487589836120605\n",
      "227 0.24250009655952454\n",
      "228 0.23065397143363953\n",
      "229 0.21938973665237427\n",
      "230 0.20870991051197052\n",
      "231 0.1986355185508728\n",
      "232 0.18900638818740845\n",
      "233 0.1798708438873291\n",
      "234 0.17117345333099365\n",
      "235 0.1628555804491043\n",
      "236 0.15498806536197662\n",
      "237 0.14754898846149445\n",
      "238 0.14044947922229767\n",
      "239 0.13368989527225494\n",
      "240 0.12723641097545624\n",
      "241 0.12113504856824875\n",
      "242 0.11530376970767975\n",
      "243 0.10975710302591324\n",
      "244 0.104521244764328\n",
      "245 0.09947852790355682\n",
      "246 0.09477350860834122\n",
      "247 0.09025387465953827\n",
      "248 0.08594129979610443\n",
      "249 0.08181910216808319\n",
      "250 0.07794135808944702\n",
      "251 0.07422475516796112\n",
      "252 0.07070398330688477\n",
      "253 0.06736490875482559\n",
      "254 0.06415736675262451\n",
      "255 0.06114181503653526\n",
      "256 0.05824127048254013\n",
      "257 0.055487290024757385\n",
      "258 0.052868399769067764\n",
      "259 0.05039536952972412\n",
      "260 0.048034779727458954\n",
      "261 0.045775726437568665\n",
      "262 0.04362001270055771\n",
      "263 0.04155861586332321\n",
      "264 0.03960896655917168\n",
      "265 0.03774574026465416\n",
      "266 0.03598983585834503\n",
      "267 0.034306760877370834\n",
      "268 0.032719433307647705\n",
      "269 0.0311797633767128\n",
      "270 0.02972240000963211\n",
      "271 0.028325002640485764\n",
      "272 0.027018139138817787\n",
      "273 0.025752568617463112\n",
      "274 0.024574793875217438\n",
      "275 0.023428454995155334\n",
      "276 0.022364851087331772\n",
      "277 0.02131631411612034\n",
      "278 0.0203352440148592\n",
      "279 0.019399123266339302\n",
      "280 0.018501749262213707\n",
      "281 0.017650222405791283\n",
      "282 0.016840271651744843\n",
      "283 0.016069378703832626\n",
      "284 0.015341577120125294\n",
      "285 0.01463576965034008\n",
      "286 0.013967320322990417\n",
      "287 0.013334476388990879\n",
      "288 0.012732330709695816\n",
      "289 0.01215207390487194\n",
      "290 0.011610647663474083\n",
      "291 0.011084601283073425\n",
      "292 0.010586553253233433\n",
      "293 0.01010966207832098\n",
      "294 0.009658082388341427\n",
      "295 0.009223049506545067\n",
      "296 0.008810139261186123\n",
      "297 0.008414991199970245\n",
      "298 0.008043451234698296\n",
      "299 0.0076879775151610374\n",
      "300 0.007344568148255348\n",
      "301 0.007023411337286234\n",
      "302 0.006719913799315691\n",
      "303 0.006428812630474567\n",
      "304 0.006140606943517923\n",
      "305 0.00586957111954689\n",
      "306 0.005617231596261263\n",
      "307 0.005379717797040939\n",
      "308 0.0051466296426951885\n",
      "309 0.004925795830786228\n",
      "310 0.004716827534139156\n",
      "311 0.004514431580901146\n",
      "312 0.0043258825317025185\n",
      "313 0.004140504635870457\n",
      "314 0.003964358940720558\n",
      "315 0.0037986435927450657\n",
      "316 0.003641005838289857\n",
      "317 0.003489870810881257\n",
      "318 0.0033428699243813753\n",
      "319 0.003208221634849906\n",
      "320 0.003077347995713353\n",
      "321 0.0029524893034249544\n",
      "322 0.0028322157450020313\n",
      "323 0.0027167419902980328\n",
      "324 0.00260699400678277\n",
      "325 0.0025044013746082783\n",
      "326 0.0024035461246967316\n",
      "327 0.002307104878127575\n",
      "328 0.0022176241036504507\n",
      "329 0.002130351960659027\n",
      "330 0.002047158544883132\n",
      "331 0.001967071322724223\n",
      "332 0.0018930885707959533\n",
      "333 0.001822647755034268\n",
      "334 0.0017526126466691494\n",
      "335 0.0016889263642951846\n",
      "336 0.0016246495069935918\n",
      "337 0.0015649936394765973\n",
      "338 0.0015051709488034248\n",
      "339 0.0014502995181828737\n",
      "340 0.001395532046444714\n",
      "341 0.001344385091215372\n",
      "342 0.0012960820458829403\n",
      "343 0.0012493499089032412\n",
      "344 0.0012063919566571712\n",
      "345 0.001163910492323339\n",
      "346 0.0011243303306400776\n",
      "347 0.0010851944098249078\n",
      "348 0.001046221237629652\n",
      "349 0.0010097033809870481\n",
      "350 0.0009771983604878187\n",
      "351 0.0009429145720787346\n",
      "352 0.0009131596307270229\n",
      "353 0.0008821353549137712\n",
      "354 0.0008533516083844006\n",
      "355 0.000825943541713059\n",
      "356 0.0007990521262399852\n",
      "357 0.0007731081568636\n",
      "358 0.0007483218796551228\n",
      "359 0.0007236443343572319\n",
      "360 0.0007022540085017681\n",
      "361 0.0006802043062634766\n",
      "362 0.0006585596711374819\n",
      "363 0.0006378818652592599\n",
      "364 0.0006187595427036285\n",
      "365 0.0005998993292450905\n",
      "366 0.0005818282952532172\n",
      "367 0.0005641243769787252\n",
      "368 0.0005478147650137544\n",
      "369 0.0005319366464391351\n",
      "370 0.0005159706925041974\n",
      "371 0.0005001361132599413\n",
      "372 0.000486206728965044\n",
      "373 0.00047238633851520717\n",
      "374 0.0004590384487528354\n",
      "375 0.0004456249298527837\n",
      "376 0.0004334429104346782\n",
      "377 0.00042151674279011786\n",
      "378 0.0004095780896022916\n",
      "379 0.0003986464289482683\n",
      "380 0.0003872691886499524\n",
      "381 0.0003757842059712857\n",
      "382 0.0003653701569419354\n",
      "383 0.0003545832587406039\n",
      "384 0.0003463330212980509\n",
      "385 0.0003367008757777512\n",
      "386 0.00032879560603760183\n",
      "387 0.00032071012537926435\n",
      "388 0.00031199678778648376\n",
      "389 0.00030405056895688176\n",
      "390 0.00029603258008137345\n",
      "391 0.00028895604191347957\n",
      "392 0.00028077512979507446\n",
      "393 0.0002737696631811559\n",
      "394 0.0002673147537279874\n",
      "395 0.0002609507355373353\n",
      "396 0.0002548137854319066\n",
      "397 0.00024909936473704875\n",
      "398 0.00024270890571642667\n",
      "399 0.0002375074109295383\n",
      "400 0.00023138831602409482\n",
      "401 0.00022606877610087395\n",
      "402 0.00022061780327931046\n",
      "403 0.00021565399947576225\n",
      "404 0.00021038709382992238\n",
      "405 0.00020536311785690486\n",
      "406 0.00020086648873984814\n",
      "407 0.00019617329235188663\n",
      "408 0.00019158511713612825\n",
      "409 0.00018720464140642434\n",
      "410 0.00018327945144847035\n",
      "411 0.00017924970597960055\n",
      "412 0.00017534343351144344\n",
      "413 0.00017151463543996215\n",
      "414 0.00016816824791021645\n",
      "415 0.00016435641737189144\n",
      "416 0.00016053044237196445\n",
      "417 0.0001576249924255535\n",
      "418 0.00015412832726724446\n",
      "419 0.00015097159484867007\n",
      "420 0.0001479009515605867\n",
      "421 0.00014484411804005504\n",
      "422 0.00014203766477294266\n",
      "423 0.00013903490616939962\n",
      "424 0.00013629236491397023\n",
      "425 0.00013365031918510795\n",
      "426 0.00013081170618534088\n",
      "427 0.00012839812552556396\n",
      "428 0.000125517908600159\n",
      "429 0.0001233514485647902\n",
      "430 0.00012071726814610884\n",
      "431 0.0001185069777420722\n",
      "432 0.00011641688615782186\n",
      "433 0.00011452529724920169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 0.00011205147893633693\n",
      "435 0.00010974762699333951\n",
      "436 0.00010784299956867471\n",
      "437 0.00010595571075100452\n",
      "438 0.00010425727668916807\n",
      "439 0.00010225272126263008\n",
      "440 0.00010055547318188474\n",
      "441 9.861459693638608e-05\n",
      "442 9.708567085908726e-05\n",
      "443 9.56562944338657e-05\n",
      "444 9.426892938790843e-05\n",
      "445 9.25842541619204e-05\n",
      "446 9.10312301130034e-05\n",
      "447 8.940070983953774e-05\n",
      "448 8.77333150128834e-05\n",
      "449 8.611906378064305e-05\n",
      "450 8.468777377856895e-05\n",
      "451 8.338928455486894e-05\n",
      "452 8.19775159470737e-05\n",
      "453 8.072082709986717e-05\n",
      "454 7.946923142299056e-05\n",
      "455 7.825759530533105e-05\n",
      "456 7.717753760516644e-05\n",
      "457 7.6169082603883e-05\n",
      "458 7.465239468729123e-05\n",
      "459 7.345811172854155e-05\n",
      "460 7.217844540718943e-05\n",
      "461 7.124476542230695e-05\n",
      "462 7.01950048096478e-05\n",
      "463 6.911564560141414e-05\n",
      "464 6.791950727347285e-05\n",
      "465 6.690211739623919e-05\n",
      "466 6.586840027011931e-05\n",
      "467 6.515279528684914e-05\n",
      "468 6.402654253179207e-05\n",
      "469 6.307702278718352e-05\n",
      "470 6.221963849384338e-05\n",
      "471 6.139762990642339e-05\n",
      "472 6.060759915271774e-05\n",
      "473 5.9766662161564454e-05\n",
      "474 5.896357470192015e-05\n",
      "475 5.8014062233269215e-05\n",
      "476 5.716370287700556e-05\n",
      "477 5.645470446324907e-05\n",
      "478 5.570526627707295e-05\n",
      "479 5.483417771756649e-05\n",
      "480 5.402691749623045e-05\n",
      "481 5.3209001634968445e-05\n",
      "482 5.2666422561742365e-05\n",
      "483 5.21078436577227e-05\n",
      "484 5.1275510486448184e-05\n",
      "485 5.0639726396184415e-05\n",
      "486 4.9951231630984694e-05\n",
      "487 4.938871643389575e-05\n",
      "488 4.86314129375387e-05\n",
      "489 4.8008132580434904e-05\n",
      "490 4.722292214864865e-05\n",
      "491 4.669440386351198e-05\n",
      "492 4.6011318772798404e-05\n",
      "493 4.54820656159427e-05\n",
      "494 4.491773870540783e-05\n",
      "495 4.433831054484472e-05\n",
      "496 4.397278098622337e-05\n",
      "497 4.352046380518004e-05\n",
      "498 4.3101648770971224e-05\n",
      "499 4.264524613972753e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow: Static Graphs\n",
    "\n",
    "- PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlow’s computational graphs are static and PyTorch uses dynamic computational graphs.\n",
    "\n",
    "- In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "- Static graphs are nice because you can optimize the graph up front; for example a framework might decide to fuse some graph operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over.\n",
    "\n",
    "- One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "- To contrast with the PyTorch autograd example above, here we use TensorFlow to fit a simple two-layer net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32715924.0\n",
      "29125668.0\n",
      "30016506.0\n",
      "30220736.0\n",
      "26611302.0\n",
      "19329942.0\n",
      "11741231.0\n",
      "6365991.5\n",
      "3421416.0\n",
      "1974623.5\n",
      "1274740.8\n",
      "913962.56\n",
      "707143.9\n",
      "573955.5\n",
      "479082.75\n",
      "406612.9\n",
      "348929.4\n",
      "301845.75\n",
      "262654.3\n",
      "229682.14\n",
      "201698.44\n",
      "177816.55\n",
      "157317.34\n",
      "139629.98\n",
      "124302.86\n",
      "110975.46\n",
      "99340.984\n",
      "89138.14\n",
      "80161.84\n",
      "72234.56\n",
      "65218.33\n",
      "58997.785\n",
      "53464.88\n",
      "48536.617\n",
      "44132.414\n",
      "40188.793\n",
      "36651.227\n",
      "33469.586\n",
      "30604.592\n",
      "28018.541\n",
      "25679.996\n",
      "23563.53\n",
      "21643.879\n",
      "19900.082\n",
      "18314.252\n",
      "16870.396\n",
      "15554.379\n",
      "14354.58\n",
      "13258.964\n",
      "12256.441\n",
      "11338.295\n",
      "10496.338\n",
      "9723.522\n",
      "9013.996\n",
      "8361.281\n",
      "7760.55\n",
      "7207.157\n",
      "6697.362\n",
      "6227.3496\n",
      "5793.7725\n",
      "5392.7993\n",
      "5021.9785\n",
      "4679.126\n",
      "4361.77\n",
      "4067.631\n",
      "3794.938\n",
      "3542.0195\n",
      "3307.3396\n",
      "3089.4895\n",
      "2887.0605\n",
      "2698.8655\n",
      "2523.8298\n",
      "2361.016\n",
      "2209.421\n",
      "2068.2983\n",
      "1936.8138\n",
      "1814.398\n",
      "1700.2499\n",
      "1593.7512\n",
      "1494.376\n",
      "1401.6581\n",
      "1315.0405\n",
      "1234.1036\n",
      "1158.4437\n",
      "1087.7001\n",
      "1021.5371\n",
      "959.66986\n",
      "901.72626\n",
      "847.4718\n",
      "796.6662\n",
      "749.0558\n",
      "704.4446\n",
      "662.65674\n",
      "623.4586\n",
      "586.69196\n",
      "552.1978\n",
      "519.8264\n",
      "489.4749\n",
      "460.99512\n",
      "434.24365\n",
      "409.12396\n",
      "385.5163\n",
      "363.3295\n",
      "342.4684\n",
      "322.86636\n",
      "304.42682\n",
      "287.08148\n",
      "270.7712\n",
      "255.4316\n",
      "240.98285\n",
      "227.38805\n",
      "214.58807\n",
      "202.53526\n",
      "191.17996\n",
      "180.49077\n",
      "170.42018\n",
      "160.93329\n",
      "151.9889\n",
      "143.55557\n",
      "135.608\n",
      "128.11452\n",
      "121.051605\n",
      "114.39124\n",
      "108.106575\n",
      "102.182144\n",
      "96.59026\n",
      "91.31315\n",
      "86.33348\n",
      "81.634094\n",
      "77.19695\n",
      "73.00813\n",
      "69.05353\n",
      "65.3183\n",
      "61.792854\n",
      "58.461655\n",
      "55.31614\n",
      "52.34233\n",
      "49.53216\n",
      "46.8779\n",
      "44.369553\n",
      "41.998245\n",
      "39.75787\n",
      "37.639416\n",
      "35.636513\n",
      "33.742752\n",
      "31.952892\n",
      "30.259579\n",
      "28.657648\n",
      "27.143074\n",
      "25.709654\n",
      "24.353836\n",
      "23.071321\n",
      "21.857151\n",
      "20.708694\n",
      "19.622393\n",
      "18.593863\n",
      "17.620182\n",
      "16.699005\n",
      "15.82636\n",
      "15.000185\n",
      "14.218363\n",
      "13.478012\n",
      "12.777177\n",
      "12.113263\n",
      "11.484162\n",
      "10.888793\n",
      "10.32482\n",
      "9.79052\n",
      "9.284185\n",
      "8.804756\n",
      "8.350196\n",
      "7.91982\n",
      "7.5119762\n",
      "7.1256514\n",
      "6.7594953\n",
      "6.412044\n",
      "6.083172\n",
      "5.770946\n",
      "5.4753933\n",
      "5.195182\n",
      "4.929646\n",
      "4.67786\n",
      "4.4390736\n",
      "4.2127247\n",
      "3.9979174\n",
      "3.794345\n",
      "3.601374\n",
      "3.4182308\n",
      "3.2444737\n",
      "3.0799818\n",
      "2.9236298\n",
      "2.7755442\n",
      "2.6349654\n",
      "2.5015292\n",
      "2.3750615\n",
      "2.2550101\n",
      "2.1411798\n",
      "2.033228\n",
      "1.9307215\n",
      "1.8334162\n",
      "1.740973\n",
      "1.6535398\n",
      "1.570449\n",
      "1.4914457\n",
      "1.4165319\n",
      "1.3455614\n",
      "1.2781006\n",
      "1.2140706\n",
      "1.1532639\n",
      "1.0955242\n",
      "1.0408612\n",
      "0.98879117\n",
      "0.93943083\n",
      "0.89255184\n",
      "0.8480138\n",
      "0.80573475\n",
      "0.76561445\n",
      "0.727545\n",
      "0.69133854\n",
      "0.6569676\n",
      "0.6243142\n",
      "0.5932274\n",
      "0.5637998\n",
      "0.5358676\n",
      "0.50928766\n",
      "0.48401785\n",
      "0.4601107\n",
      "0.43737647\n",
      "0.41572097\n",
      "0.3951466\n",
      "0.3756398\n",
      "0.35709956\n",
      "0.3395157\n",
      "0.32271808\n",
      "0.30683762\n",
      "0.29169443\n",
      "0.27739155\n",
      "0.26369512\n",
      "0.2507259\n",
      "0.2383742\n",
      "0.22670281\n",
      "0.2155607\n",
      "0.20498216\n",
      "0.19493681\n",
      "0.18534698\n",
      "0.17627421\n",
      "0.16766939\n",
      "0.15942816\n",
      "0.15166736\n",
      "0.14422627\n",
      "0.13717496\n",
      "0.13050546\n",
      "0.124129936\n",
      "0.11806595\n",
      "0.112297565\n",
      "0.106836215\n",
      "0.10161595\n",
      "0.09667083\n",
      "0.0919781\n",
      "0.08752129\n",
      "0.08325733\n",
      "0.0792268\n",
      "0.075370304\n",
      "0.071722664\n",
      "0.0682218\n",
      "0.06493113\n",
      "0.061763152\n",
      "0.05876489\n",
      "0.055933237\n",
      "0.053246565\n",
      "0.05067902\n",
      "0.04821309\n",
      "0.0458998\n",
      "0.043686077\n",
      "0.041582063\n",
      "0.039586306\n",
      "0.037692674\n",
      "0.035880767\n",
      "0.03415417\n",
      "0.03250021\n",
      "0.030934155\n",
      "0.029444605\n",
      "0.02803323\n",
      "0.026696704\n",
      "0.025419993\n",
      "0.024202073\n",
      "0.023045884\n",
      "0.021937555\n",
      "0.020888569\n",
      "0.019894343\n",
      "0.018958863\n",
      "0.018058836\n",
      "0.017202303\n",
      "0.016386084\n",
      "0.015605449\n",
      "0.014864313\n",
      "0.014168203\n",
      "0.013499708\n",
      "0.012863908\n",
      "0.01226503\n",
      "0.011683582\n",
      "0.011142644\n",
      "0.010623244\n",
      "0.010126751\n",
      "0.009653554\n",
      "0.0092049865\n",
      "0.008778442\n",
      "0.0083702225\n",
      "0.00798077\n",
      "0.007618284\n",
      "0.007263422\n",
      "0.006930138\n",
      "0.006614575\n",
      "0.006316241\n",
      "0.006028129\n",
      "0.0057564187\n",
      "0.005498826\n",
      "0.0052549383\n",
      "0.0050175134\n",
      "0.0047923876\n",
      "0.004578099\n",
      "0.0043789125\n",
      "0.0041854167\n",
      "0.003999423\n",
      "0.003825753\n",
      "0.003660124\n",
      "0.0035010725\n",
      "0.003353985\n",
      "0.0032043832\n",
      "0.0030674983\n",
      "0.0029363926\n",
      "0.0028129725\n",
      "0.0026941202\n",
      "0.002580497\n",
      "0.002474505\n",
      "0.0023706877\n",
      "0.0022732646\n",
      "0.0021786578\n",
      "0.0020905696\n",
      "0.0020044001\n",
      "0.0019262197\n",
      "0.0018496643\n",
      "0.001777223\n",
      "0.0017065598\n",
      "0.001641956\n",
      "0.0015782127\n",
      "0.001515866\n",
      "0.0014601158\n",
      "0.0014039228\n",
      "0.0013477592\n",
      "0.0012993579\n",
      "0.0012510542\n",
      "0.0012053458\n",
      "0.0011611079\n",
      "0.0011202567\n",
      "0.001078628\n",
      "0.0010391383\n",
      "0.001002143\n",
      "0.0009669704\n",
      "0.0009320403\n",
      "0.00089997443\n",
      "0.00086882414\n",
      "0.00083889876\n",
      "0.00080941437\n",
      "0.00078337826\n",
      "0.00075561256\n",
      "0.0007305628\n",
      "0.0007065645\n",
      "0.00068369263\n",
      "0.00066301756\n",
      "0.00064126507\n",
      "0.00062178326\n",
      "0.00060159713\n",
      "0.00058456784\n",
      "0.00056504575\n",
      "0.00054672325\n",
      "0.0005306981\n",
      "0.0005145703\n",
      "0.00049861346\n",
      "0.00048405142\n",
      "0.0004693522\n",
      "0.00045534817\n",
      "0.00044125755\n",
      "0.0004283113\n",
      "0.00041601117\n",
      "0.00040430386\n",
      "0.00039254702\n",
      "0.00038187634\n",
      "0.00037085408\n",
      "0.00036085796\n",
      "0.00035069435\n",
      "0.0003417802\n",
      "0.00033245498\n",
      "0.00032350194\n",
      "0.00031446925\n",
      "0.00030557037\n",
      "0.00029843143\n",
      "0.00028996804\n",
      "0.00028251007\n",
      "0.0002757337\n",
      "0.0002685688\n",
      "0.00026126803\n",
      "0.00025530878\n",
      "0.0002493094\n",
      "0.00024333791\n",
      "0.00023704869\n",
      "0.00023087968\n",
      "0.00022543056\n",
      "0.00022034545\n",
      "0.00021451089\n",
      "0.00020984915\n",
      "0.00020501512\n",
      "0.00020009547\n",
      "0.0001957438\n",
      "0.00019093015\n",
      "0.00018644093\n",
      "0.00018227863\n",
      "0.00017804808\n",
      "0.00017440805\n",
      "0.00017025325\n",
      "0.00016640083\n",
      "0.00016288077\n",
      "0.00015941296\n",
      "0.00015577866\n",
      "0.00015269153\n",
      "0.00014943106\n",
      "0.00014656976\n",
      "0.00014350719\n",
      "0.00014040979\n",
      "0.00013770611\n",
      "0.00013464934\n",
      "0.00013193437\n",
      "0.00012898476\n",
      "0.00012664002\n",
      "0.00012470318\n",
      "0.00012186474\n",
      "0.00011972955\n",
      "0.00011687605\n",
      "0.00011454667\n",
      "0.000112554255\n",
      "0.00011000799\n",
      "0.00010785121\n",
      "0.00010569544\n",
      "0.00010401932\n",
      "0.00010239328\n",
      "0.000100313264\n",
      "9.839189e-05\n",
      "9.654524e-05\n",
      "9.44944e-05\n",
      "9.246345e-05\n",
      "9.098175e-05\n",
      "8.89715e-05\n",
      "8.782382e-05\n",
      "8.600559e-05\n",
      "8.473297e-05\n",
      "8.334572e-05\n",
      "8.174485e-05\n",
      "8.016559e-05\n",
      "7.906157e-05\n",
      "7.752131e-05\n",
      "7.623376e-05\n",
      "7.4723604e-05\n",
      "7.388866e-05\n",
      "7.267696e-05\n",
      "7.118937e-05\n",
      "7.056183e-05\n",
      "6.892454e-05\n",
      "6.787045e-05\n",
      "6.681338e-05\n",
      "6.5882625e-05\n",
      "6.499605e-05\n",
      "6.418037e-05\n",
      "6.29207e-05\n",
      "6.1908955e-05\n",
      "6.103874e-05\n",
      "6.0128856e-05\n",
      "5.9335438e-05\n",
      "5.8269005e-05\n",
      "5.7239682e-05\n",
      "5.6421668e-05\n",
      "5.5625114e-05\n",
      "5.468097e-05\n",
      "5.4130538e-05\n",
      "5.324812e-05\n",
      "5.2381794e-05\n",
      "5.1746538e-05\n",
      "5.0963987e-05\n",
      "5.0108283e-05\n",
      "4.9420563e-05\n",
      "4.8804635e-05\n",
      "4.795966e-05\n",
      "4.7373745e-05\n",
      "4.6831417e-05\n",
      "4.6302805e-05\n",
      "4.5656692e-05\n",
      "4.4913115e-05\n",
      "4.4170676e-05\n",
      "4.3713935e-05\n",
      "4.3052405e-05\n",
      "4.244205e-05\n"
     ]
    }
   ],
   "source": [
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
